{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d57fa57c",
   "metadata": {},
   "source": [
    "# Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97daadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b747e6",
   "metadata": {},
   "source": [
    "# List Files Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f516df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c507da67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english-hindi-all-linked.tsv',\n",
       " 'english-hindi-assamese-linked.tsv',\n",
       " 'english-hindi-bengali-gujarati-kannada-malayalam-punjabi-telugu-tamil-urdu-linked.tsv',\n",
       " 'english-hindi-bengali-linked.tsv',\n",
       " 'english-hindi-bodo-linked.tsv',\n",
       " 'english-hindi-gujarati-linked.tsv',\n",
       " 'english-hindi-kannada-linked.tsv',\n",
       " 'english-hindi-kashmiri-linked.tsv',\n",
       " 'english-hindi-konkani-linked.tsv',\n",
       " 'english-hindi-linked.tsv',\n",
       " 'english-hindi-malayalam-linked.tsv',\n",
       " 'english-hindi-manipuri-linked.tsv',\n",
       " 'english-hindi-marathi-linked.tsv',\n",
       " 'english-hindi-nepali-linked.tsv',\n",
       " 'english-hindi-oriya-linked.tsv',\n",
       " 'english-hindi-punjabi-linked.tsv',\n",
       " 'english-hindi-sanskrit-linked.tsv',\n",
       " 'english-hindi-tamil-linked.tsv',\n",
       " 'english-hindi-telugu-linked.tsv',\n",
       " 'english-hindi-urdu-linked.tsv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8111efd",
   "metadata": {},
   "source": [
    "# Pre-Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dfc7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSynsetColumns(df):\n",
    "    synsetColumns = []\n",
    "    for column in list(df.columns):\n",
    "        if \"_synset\" in column:\n",
    "            synsetColumns.append(column)\n",
    "    return synsetColumns\n",
    "\n",
    "def getGlossColumns(df):\n",
    "    synsetColumns = []\n",
    "    for column in list(df.columns):\n",
    "        if \"_gloss\" in column:\n",
    "            synsetColumns.append(column)\n",
    "    return synsetColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb82da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitWord(word):\n",
    "    if \"_\" in word:\n",
    "        return \" \".join(word.split(\"_\"))\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b231ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTags(word):\n",
    "    word = word.replace(\"(a)\",\"\")\n",
    "    word = word.replace(\"(p)\",\"\")\n",
    "    word = word.replace(\"(ip)\",\"\")\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce38ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAlpha(word):\n",
    "    return word.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff6e31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasDigit(s):\n",
    "    return any(i.isdigit() for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87f35822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isRomanNumeral(word):\n",
    "    upperRoman = bool(re.search(\"^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\",word))\n",
    "    lowerRoman = bool(re.search(\"^m{0,3}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})$\",word))\n",
    "    return upperRoman ^ lowerRoman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42cd62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processWord(word):\n",
    "    word = splitWord(word)\n",
    "    word = removeTags(word)\n",
    "    if len(word)>1 and word.isnumeric()!=True and hasDigit(word)!=True and isRomanNumeral(word)!=True:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b588f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNan(word):\n",
    "    try:\n",
    "        isNanObject = float(word)\n",
    "        return math.isnan(isNanObject)\n",
    "    except:return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98432f",
   "metadata": {},
   "source": [
    "# Load the TSVs into Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc8f062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.27it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dict = {}\n",
    "for file in tqdm.tqdm(os.listdir(data_dir)):\n",
    "    filename = file.split(\".\")[0]\n",
    "    data_dict[filename] = pd.read_csv(os.path.join(data_dir,file),sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f89d4e",
   "metadata": {},
   "source": [
    "# Processing function for Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a1debd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processIndoDf(df):\n",
    "    new_df = df.copy(deep=True)\n",
    "    requiredColumns = []\n",
    "    synsetColumns = getSynsetColumns(new_df)\n",
    "    glossColumns = getGlossColumns(new_df)\n",
    "    requiredColumns+=synsetColumns\n",
    "    requiredColumns+=glossColumns\n",
    "    try:\n",
    "        new_df = new_df[requiredColumns+[\"english_category\"]]\n",
    "    except:\n",
    "        new_df = new_df[requiredColumns+[\"english_category_x\"]]\n",
    "        new_df = new_df.rename(columns={\"english_category_x\":\"english_category\"})\n",
    "    \n",
    "    for synsetColumn in synsetColumns:\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:str(x))\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:x.strip())\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:x.split(\",\"))\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:[y.strip() for y in x])\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:[processWord(y) for y in x])\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:[y for y in x if y is not None])\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:[y for y in x if isNan(y)==False])\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:[y for y in x if y!=\"\"])\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:[y.lower() for y in x])\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:[y.replace('\"','') for y in x])\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:[y.replace(\"\\\\\",\"\") for y in x])\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:[y.replace(\".\",\"\") for y in x])\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:[y.replace(\";\",\"\") for y in x])\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:[y.lower() for y in x])\n",
    "        new_df[synsetColumn] = new_df[synsetColumn].apply(lambda x:\";\".join(x))\n",
    "        \n",
    "    for glossColumn in glossColumns:\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:str(x))\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:x.replace(',',\";\"))\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:x.replace('|',\";\"))\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:x.split(\";\"))\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:[y.replace('\"',\"\") for y in x])\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:[y.replace('।',\"\") for y in x])\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:[y.replace('.',\"\") for y in x])\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:[y.replace(';',\"\") for y in x])\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:[y.strip() for y in x])\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:[y for y in x if y is not None])\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:[y for y in x if y!=\"\"])\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:[y for y in x if isNan(y)==False])\n",
    "        new_df[glossColumn] = new_df[glossColumn].apply(lambda x:\";\".join(x))\n",
    "        \n",
    "    requiredColumns.remove(\"english_synset_words\")\n",
    "    \n",
    "    aggMap = {}\n",
    "    for column in requiredColumns:\n",
    "        aggMap[column] = \";\".join\n",
    "    \n",
    "    requiredColumns.remove(\"english_gloss\")\n",
    "    requiredColumns.sort()\n",
    "        \n",
    "    new_df = new_df.groupby([\"english_synset_words\",\"english_category\"]).agg(aggMap).reset_index()\n",
    "    new_df = new_df[[\"english_synset_words\",\"english_category\",\"english_gloss\"]+requiredColumns]\n",
    "    new_df = new_df.rename(columns={\"english_synset_words\":\"english_word\"})\n",
    "    new_df = new_df.rename(columns={\"english_category\":\"pos\"})\n",
    "    new_df[\"english_word\"] = new_df[\"english_word\"].apply(lambda x:x.split(';'))\n",
    "    new_df = new_df.explode([\"english_word\"])\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cad4b",
   "metadata": {},
   "source": [
    "# Process each dataframe in the main dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b4635a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [01:17<00:00,  3.86s/it]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm.tqdm(data_dict.keys()):\n",
    "    data_dict[key] = processIndoDf(data_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f09ea8",
   "metadata": {},
   "source": [
    "# Save the processed Dataframes as TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16d420b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:08<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm.tqdm(data_dict.keys()):\n",
    "    data_dict[key].to_csv(f\"processed/tsv/{key}.tsv\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed99bd",
   "metadata": {},
   "source": [
    "# Save the Dataframes as json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e61d8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfToJson(df,filename):\n",
    "    data_json = {}\n",
    "    for word,row in df.iterrows():\n",
    "        if word not in data_json:\n",
    "            data_json[word] = []\n",
    "        data_json[word].append(row.to_dict())\n",
    "    with open(filename,'w',encoding=\"utf-8\") as f:\n",
    "        json.dump(data_json,f,indent=4,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1f5d392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:36<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm.tqdm(data_dict.keys()):\n",
    "    data_dict[key]=data_dict[key].set_index(\"english_word\")\n",
    "    dfToJson(data_dict[key],f\"processed/json/{key}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21395e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
